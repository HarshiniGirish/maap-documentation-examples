{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "%pip install geopandas\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import requests\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up cmr hosts\n",
    "NASA_HOST = \"cmr.earthdata.nasa.gov\"\n",
    "MAAP_HOST = \"cmr.maap-project.org\"\n",
    "\n",
    "# Reading the indices for boreal and copernicus\n",
    "dem = gpd.read_file(\"dem30m_tiles.geojson\")\n",
    "boreal = gpd.read_file(\"boreal_grid_albers90k_gpkg.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_from_results(response, format, seen):\n",
    "    '''\n",
    "        Based on the response from the post call, format and the seen list,\n",
    "        gives the total size and num granules from the response\n",
    "    '''\n",
    "    if format == \"umm_json\":\n",
    "        results = response[\"items\"]\n",
    "        unique = [result for result in results if result[\"umm\"][\"GranuleUR\"] not in seen]\n",
    "        total_size = sum(\n",
    "            [\n",
    "                float(\n",
    "                    granule[\"umm\"][\"DataGranule\"][\"ArchiveAndDistributionInformation\"][0][\"SizeInBytes\"]\n",
    "                )\n",
    "                for granule in unique\n",
    "            ]\n",
    "        )\n",
    "        num_granules = len(unique)\n",
    "        seen.extend([result[\"umm\"][\"GranuleUR\"] for result in results])\n",
    "    else:\n",
    "        results = response[\"feed\"][\"entry\"]\n",
    "        total_size = sum([float(result[\"granule_size\"]) for result in results])\n",
    "        num_granules = len(results)\n",
    "\n",
    "    return total_size, num_granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_estimates(query_dict, host=\"maap\", format=\"json\", seen=[]):\n",
    "    '''\n",
    "        Based on the query params (query_dict) to cmr (host: maap or nasa), format and seen list (for de-duplication),\n",
    "        gives size estimates\n",
    "    '''\n",
    "    host = MAAP_HOST if host == \"maap\" else NASA_HOST\n",
    "    base_url = f\"https://{host}/search/granules.{format}\"\n",
    "\n",
    "    headers = {}\n",
    "\n",
    "    total_size = 0\n",
    "    num_granules = 0\n",
    "\n",
    "    while True:\n",
    "        if num_granules and (num_granules % 1000 == 0):\n",
    "            print(f\"{num_granules} granules processed\")\n",
    "\n",
    "        response = requests.post(base_url, data=query_dict, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            size, num_grans = get_size_from_results(response.json(), format, seen)\n",
    "            total_size += size\n",
    "            num_granules += num_grans\n",
    "\n",
    "            if search_after := response.headers.get(\"CMR-Search-After\"):\n",
    "                headers = {\"CMR-Search-After\": search_after}\n",
    "            else:\n",
    "                print(\"No more granules\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Response status code:\", response.status_code)\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "    return {\"total_size\": total_size, \"num_granules\": num_granules}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 1 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel estimates\n",
    "sentinel_dict = {\n",
    "    \"collection_concept_id\": \"C1214470488-ASF\",\n",
    "    \"pageSize\": 2000,\n",
    "    \"temporal\": \"2019-01-01T00:00:00Z,2019-12-31T23:59:59Z\",\n",
    "}\n",
    "\n",
    "print(\n",
    "    get_size_estimates(sentinel_dict, host=\"nasa\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATL08 v5 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATL08 v5 estimates\n",
    "atl08_dict = {\n",
    "    'collection_concept_id': \"C1201746153-NASA_MAAP\",\n",
    "    'pageSize': 2000,\n",
    "    'temporal': '2019-06-01T00:00:00Z,2019-09-30T23:59:59Z',\n",
    "    'bounding_box': '-180,50,180,75',\n",
    "    'provider': 'NASA_MAAP'\n",
    "}\n",
    "print(\n",
    "    get_size_estimates(atl08_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATL03 v4 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATL03 v4 estimates\n",
    "atl03_dict = {\n",
    "    'collection_concept_id': \"C1201300747-NASA_MAAP\",\n",
    "    'pageSize': 2000,\n",
    "    'temporal': '2019-06-01T00:00:00Z,2019-09-30T23:59:59Z',\n",
    "    'bounding_box': '-180,50,180,75',\n",
    "    'provider': 'NASA_MAAP'\n",
    "}\n",
    "print(\n",
    "    get_size_estimates(atl03_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copernicus DEM estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copernicus_s3_list():\n",
    "    '''\n",
    "        Gets all the s3 files from the dem index that intersect with the boreal index\n",
    "    '''\n",
    "    selection = dem[dem.intersects(boreal.to_crs(\"EPSG:4326\").unary_union)]\n",
    "    return selection[\"s3\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_aws(list_of_urls):\n",
    "    '''\n",
    "        Gets the cumulative size of all the files from the list_of_urls (s3 urls)\n",
    "    '''\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    total_size = 0\n",
    "    count = 0\n",
    "    for url in tqdm(list_of_urls):\n",
    "        split_url = url.replace(\"s3://\", \"\").split(\"/\")\n",
    "        bucket, key = split_url[0], \"/\".join(split_url[1:])\n",
    "        response = s3.head_object(Bucket=bucket, Key=key)\n",
    "        total_size += response[\"ContentLength\"]\n",
    "        count += 1\n",
    "    return total_size, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copernicus DEM estimates\n",
    "print(\n",
    "    get_size_aws(\n",
    "        get_copernicus_s3_list()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLS v2 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_polygon(polygon):\n",
    "    '''\n",
    "        CMR supports counter-clockwise polygons as query params, this function converts the clock-wise polygons that geopandas gives to counterclockwise polygons that cmr accepts\n",
    "    '''\n",
    "    reversed = []\n",
    "    for index in range(len(polygon)-1, -1, -2):\n",
    "        reversed.append(polygon[index-1])\n",
    "        reversed.append(polygon[index])\n",
    "    return \",\".join(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boreal_polygons():\n",
    "    '''\n",
    "        Get all the polygons bounds from boreal index, used to make calls to cmr\n",
    "    '''\n",
    "    polygons = []\n",
    "    float_regex = \"[+-]?[0-9]*[.][0-9]+\"\n",
    "    for polygon in boreal.to_crs(\"EPSG:4326\")[\"geometry\"]:\n",
    "        long_lats = re.findall(float_regex, f\"{str(polygon)}\")\n",
    "        polygons.append(_reverse_polygon(long_lats))\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLS v2 estimates\n",
    "polygons = get_boreal_polygons()\n",
    "hls_dict = {\n",
    "    \"collection_concept_id\": \"C2021957295-LPCLOUD\",\n",
    "    \"pageSize\": 2000\n",
    "}\n",
    "\n",
    "hls_seen_granules = []\n",
    "total_size = 0\n",
    "num_granules = 0\n",
    "\n",
    "for year in range(2019, 2022):\n",
    "    for polygon in tqdm(polygons):\n",
    "        hls_dict[\"polygon\"] = polygon\n",
    "        hls_dict[\"temporal\"] = f\"{year}-06-01T00:00:00Z,{year}-09-15T23:59:59Z\",\n",
    "        result = get_size_estimates(hls_dict, host=\"nasa\", format=\"umm_json\", seen=hls_seen_granules)\n",
    "\n",
    "        total_size += result[\"total_size\"]\n",
    "        num_granules += result[\"num_granules\"]\n",
    "\n",
    "print( {\n",
    "    \"total_size\": total_size,\n",
    "    \"num_granules\": num_granules\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
