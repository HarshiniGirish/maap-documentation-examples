{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.3.0-cp39-cp39-macosx_10_9_x86_64.whl (7.7 MB)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from geopandas) (1.3.4)\n",
      "Collecting shapely>=1.6\n",
      "  Using cached Shapely-1.8.1.post1-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "Collecting fiona>=1.8\n",
      "  Using cached Fiona-1.8.21-cp39-cp39-macosx_10_10_x86_64.whl (18.5 MB)\n",
      "Requirement already satisfied: setuptools in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: certifi in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
      "Requirement already satisfied: attrs>=17 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six>=1.7 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: click>=4.0 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
      "Installing collected packages: munch, shapely, pyproj, fiona, geopandas\n",
      "Successfully installed fiona-1.8.21 geopandas-0.10.2 munch-2.5.0 pyproj-3.3.0 shapely-1.8.1.post1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/sadhikar/repo/MAAP/playground/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages (4.62.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/sadhikar/repo/MAAP/playground/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "%pip install geopandas\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhikar/repo/MAAP/playground/env/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "import requests\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up cmr hosts\n",
    "NASA_HOST = \"cmr.earthdata.nasa.gov\"\n",
    "MAAP_HOST = \"cmr.maap-project.org\"\n",
    "\n",
    "# Reading the indices for boreal and copernicus\n",
    "dem = gpd.read_file(\"dem30m_tiles.geojson\")\n",
    "boreal = gpd.read_file(\"boreal_grid_albers90k_gpkg.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_from_results(response, format, seen):\n",
    "    '''\n",
    "        Based on the response from the post call, format and the seen list,\n",
    "        gives the total size and num granules from the response\n",
    "\n",
    "        :param response: response from the post call to the cmr\n",
    "        :param format: format of the data (json or umm_json)\n",
    "        :param seen: list of seen granules (to avoid duplicates)\n",
    "\n",
    "        :return: total size in bytes and number of granules\n",
    "    '''\n",
    "\n",
    "    # HLS requires umm_json because the default json doesn't have the size of the granule\n",
    "    if format == \"umm_json\":\n",
    "        results = response[\"items\"]\n",
    "        # Only keep the granules whose `GranuleUR` is not in the seen list\n",
    "        unique = [result for result in results if result[\"umm\"][\"GranuleUR\"] not in seen]\n",
    "        total_size = sum(\n",
    "            [\n",
    "                float(\n",
    "                    granule[\"umm\"][\"DataGranule\"][\"ArchiveAndDistributionInformation\"][0][\"SizeInBytes\"]\n",
    "                )\n",
    "                for granule in unique\n",
    "            ]\n",
    "        )\n",
    "        num_granules = len(unique)\n",
    "        # Add the granuleUR to the seen list\n",
    "        seen.extend([result[\"umm\"][\"GranuleUR\"] for result in unique])\n",
    "    else:\n",
    "        # For the non-HLS case, the size is in the json response\n",
    "        # We repeat the same process as above, but with the json response\n",
    "        results = response[\"feed\"][\"entry\"]\n",
    "        unique = [result for result in results if result[\"producer_granule_id\"] not in seen]\n",
    "\n",
    "        total_size = sum([float(result[\"granule_size\"]) for result in unique])\n",
    "        num_granules = len(unique)\n",
    "\n",
    "        seen.extend([result[\"producer_granule_id\"] for result in unique])\n",
    "\n",
    "    return total_size, num_granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_estimates(query_dict, host=\"maap\", format=\"json\", seen=[]):\n",
    "    '''\n",
    "        Based on the query params (query_dict) to cmr (host: maap or nasa), format and seen list (for de-duplication),\n",
    "        gives size estimates\n",
    "\n",
    "        :param query_dict: dict of query params that the cmr accepts\n",
    "        :param host: cmr host (maap or nasa)\n",
    "        :param format: format of the data (json or umm_json)\n",
    "        :param seen: list of seen granuleids (to avoid duplicates)\n",
    "\n",
    "        :return: dict of total_size (in bytes) and num_granules\n",
    "    '''\n",
    "    # Setting the host and url based on the inputs\n",
    "    host = MAAP_HOST if host == \"maap\" else NASA_HOST\n",
    "    base_url = f\"https://{host}/search/granules.{format}\"\n",
    "\n",
    "    headers = {}\n",
    "\n",
    "    # Initialize the size and count\n",
    "    total_size = 0\n",
    "    num_granules = 0\n",
    "\n",
    "    # The cmr results are paginated, so we need to loop through the pages until all the granules are accounted for\n",
    "    while True:\n",
    "        if num_granules and (num_granules % 1000 == 0):\n",
    "            print(f\"{num_granules} granules processed\")\n",
    "\n",
    "        # Making the post call to cmr\n",
    "        response = requests.post(base_url, data=query_dict, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Calculate size and granule count and add them up\n",
    "            size, num_grans = get_size_from_results(response.json(), format, seen)\n",
    "            total_size += size\n",
    "            num_granules += num_grans\n",
    "        \n",
    "        # If there's more granules in the next page, update header and run again, else break\n",
    "            if search_after := response.headers.get(\"CMR-Search-After\"):\n",
    "                headers = {\"CMR-Search-After\": search_after}\n",
    "            else:\n",
    "                print(\"No more granules\")\n",
    "                break\n",
    "        # If the response is not 200, print the error and break\n",
    "        else:\n",
    "            print(\"Response status code:\", response.status_code)\n",
    "            print(response.text)\n",
    "            break\n",
    "\n",
    "    return {\"total_size\": total_size, \"count\": num_granules}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 1 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 granules processed\n",
      "4000 granules processed\n",
      "6000 granules processed\n",
      "8000 granules processed\n",
      "10000 granules processed\n",
      "12000 granules processed\n",
      "14000 granules processed\n",
      "16000 granules processed\n",
      "18000 granules processed\n",
      "20000 granules processed\n",
      "22000 granules processed\n",
      "24000 granules processed\n",
      "26000 granules processed\n",
      "28000 granules processed\n",
      "30000 granules processed\n",
      "32000 granules processed\n",
      "34000 granules processed\n",
      "36000 granules processed\n",
      "38000 granules processed\n",
      "40000 granules processed\n",
      "42000 granules processed\n",
      "44000 granules processed\n",
      "46000 granules processed\n",
      "48000 granules processed\n",
      "50000 granules processed\n",
      "52000 granules processed\n",
      "54000 granules processed\n",
      "56000 granules processed\n",
      "58000 granules processed\n",
      "60000 granules processed\n",
      "62000 granules processed\n",
      "64000 granules processed\n",
      "66000 granules processed\n",
      "68000 granules processed\n",
      "70000 granules processed\n",
      "72000 granules processed\n",
      "74000 granules processed\n",
      "76000 granules processed\n",
      "78000 granules processed\n",
      "80000 granules processed\n",
      "82000 granules processed\n",
      "84000 granules processed\n",
      "86000 granules processed\n",
      "88000 granules processed\n",
      "90000 granules processed\n",
      "92000 granules processed\n",
      "94000 granules processed\n",
      "96000 granules processed\n",
      "98000 granules processed\n",
      "100000 granules processed\n",
      "102000 granules processed\n",
      "104000 granules processed\n",
      "106000 granules processed\n",
      "108000 granules processed\n",
      "110000 granules processed\n",
      "112000 granules processed\n",
      "114000 granules processed\n",
      "116000 granules processed\n",
      "118000 granules processed\n",
      "120000 granules processed\n",
      "122000 granules processed\n",
      "124000 granules processed\n",
      "126000 granules processed\n",
      "128000 granules processed\n",
      "130000 granules processed\n",
      "132000 granules processed\n",
      "134000 granules processed\n",
      "136000 granules processed\n",
      "138000 granules processed\n",
      "140000 granules processed\n",
      "142000 granules processed\n",
      "144000 granules processed\n",
      "146000 granules processed\n",
      "148000 granules processed\n",
      "150000 granules processed\n",
      "152000 granules processed\n",
      "154000 granules processed\n",
      "156000 granules processed\n",
      "158000 granules processed\n",
      "160000 granules processed\n",
      "162000 granules processed\n",
      "164000 granules processed\n",
      "166000 granules processed\n",
      "168000 granules processed\n",
      "170000 granules processed\n",
      "172000 granules processed\n",
      "174000 granules processed\n",
      "176000 granules processed\n",
      "178000 granules processed\n",
      "180000 granules processed\n",
      "182000 granules processed\n",
      "184000 granules processed\n",
      "186000 granules processed\n",
      "188000 granules processed\n",
      "190000 granules processed\n",
      "192000 granules processed\n",
      "194000 granules processed\n",
      "196000 granules processed\n",
      "No more granules\n",
      "{'total_size': 798677867.307477, 'count': 196520}\n"
     ]
    }
   ],
   "source": [
    "# Build the query params for Sentinel 1 to send to cmr, with the given temporal range\n",
    "sentinel_dict = {\n",
    "    \"collection_concept_id\": \"C1214470488-ASF\",\n",
    "    \"pageSize\": 2000,\n",
    "    \"temporal\": \"2019-01-01T00:00:00Z,2019-12-31T23:59:59Z\",\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the nasa cmr (thus, host=\"nasa\")\n",
    "sentinel_estimates = get_size_estimates(sentinel_dict, host=\"nasa\")\n",
    "print(sentinel_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATL08 v5 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 granules processed\n",
      "4000 granules processed\n",
      "No more granules\n",
      "{'total_size': 382292.7138280845, 'count': 5882}\n"
     ]
    }
   ],
   "source": [
    "# Build the query params for ATL08 v5 to send to cmr, with the given temporal range and bounding box\n",
    "atl08_dict = {\n",
    "    'collection_concept_id': \"C1201746153-NASA_MAAP\",\n",
    "    'pageSize': 2000,\n",
    "    'temporal': '2019-06-01T00:00:00Z,2019-09-30T23:59:59Z',\n",
    "    'bounding_box': '-180,50,180,75',\n",
    "    'provider': 'NASA_MAAP'\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the maap cmr (host is defaulted to \"maap\")\n",
    "atl08_estimates = get_size_estimates(atl08_dict)\n",
    "print(atl08_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATL03 v4 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 granules processed\n",
      "4000 granules processed\n",
      "6000 granules processed\n",
      "No more granules\n",
      "{'total_size': 11310057.160719872, 'count': 6593}\n"
     ]
    }
   ],
   "source": [
    "# Build the query params for ATL03 v4 to send to cmr, with the given temporal range and bounding box\n",
    "atl03_dict = {\n",
    "    'collection_concept_id': \"C1201300747-NASA_MAAP\",\n",
    "    'pageSize': 2000,\n",
    "    'temporal': '2019-06-01T00:00:00Z,2019-09-30T23:59:59Z',\n",
    "    'bounding_box': '-180,50,180,75',\n",
    "    'provider': 'NASA_MAAP'\n",
    "}\n",
    "\n",
    "# Call the get_size_estimates function to get the size estimates\n",
    "# The data is hosted in the maap cmr (host is defaulted to \"maap\")\n",
    "atl03_estimates = get_size_estimates(atl03_dict)\n",
    "print(atl03_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copernicus DEM estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copernicus_s3_list():\n",
    "    '''\n",
    "        Gets all the s3 files from the dem index that intersect with the boreal index\n",
    "    '''\n",
    "    # Get the intersection of indices from boreal and copernicus dem\n",
    "    selection = dem[dem.intersects(boreal.to_crs(\"EPSG:4326\").unary_union)]\n",
    "    # Get all the s3 file urls from the intersection\n",
    "    return selection[\"s3\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_aws(list_of_urls):\n",
    "    '''\n",
    "        Gets the cumulative size of all the files from the list_of_urls (s3 urls)\n",
    "\n",
    "        :param: list_of_urls: list of s3 urls in the form s3://<bucket>/<path to file>\n",
    "\n",
    "        :return: total size in bytes and count of files\n",
    "    '''\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    total_size = 0\n",
    "    count = 0\n",
    "    for url in tqdm(list_of_urls):\n",
    "        # Get bucket and key from the s3:// url\n",
    "        split_url = url.replace(\"s3://\", \"\").split(\"/\")\n",
    "        bucket, key = split_url[0], \"/\".join(split_url[1:])\n",
    "\n",
    "        # Make a head request to get the size of the file\n",
    "        response = s3.head_object(Bucket=bucket, Key=key)\n",
    "\n",
    "        # Add to size and increment count\n",
    "        total_size += response[\"ContentLength\"]\n",
    "        count += 1\n",
    "\n",
    "    return {\n",
    "        \"total_size\": total_size,\n",
    "        \"count\": count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5994/5994 [15:27<00:00,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_size': 140900308827, 'count': 5994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calls the get_size_aws function to get the size estimates for all the s3 urls\n",
    "copernicus_estimates = get_size_aws(\n",
    "    # Get the list of s3 urls from the dem intersection boreal indices\n",
    "    get_copernicus_s3_list()\n",
    ")\n",
    "print(copernicus_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLS v2 estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reverse_polygon(polygon):\n",
    "    '''\n",
    "        CMR supports counter-clockwise polygons as query params, this function converts the clock-wise polygons that geopandas gives to counterclockwise polygons that cmr accepts\n",
    "\n",
    "        :param polygon: polygon in the form [x1, y1, x2, y2, ...] in clockwise order\n",
    "\n",
    "        :param: polygon in the form \"x1,y1,x2,y2,...\" in counterclockwise order (format that cmr accepts)\n",
    "    '''\n",
    "    reversed = []\n",
    "    # Read the polygon from end to start\n",
    "    for index in range(len(polygon)-1, -1, -2):\n",
    "        # Add a pair of coordinates to the reversed list\n",
    "        reversed.append(polygon[index-1])\n",
    "        reversed.append(polygon[index])\n",
    "    return \",\".join(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boreal_polygons():\n",
    "    '''\n",
    "        Get all the polygons bounds from boreal index, used to make calls to cmr\n",
    "    '''\n",
    "    polygons = []\n",
    "    # Regex to find all the floating point numbers in a string\n",
    "    float_regex = \"[+-]?[0-9]*[.][0-9]+\"\n",
    "\n",
    "    for polygon in boreal.to_crs(\"EPSG:4326\")[\"geometry\"]:\n",
    "        # Finds all the floating point numbers (which are coordinates) in the polygon\n",
    "        long_lats = re.findall(float_regex, f\"{str(polygon)}\")\n",
    "        # Reverses them and adds to the polygons list\n",
    "        polygons.append(_reverse_polygon(long_lats))\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the polygons from the boreal index\n",
    "polygons = get_boreal_polygons()\n",
    "\n",
    "# Build query params for the cmr call for HLS v2\n",
    "hls_dict = {\n",
    "    \"collection_concept_id\": \"C2021957295-LPCLOUD\",\n",
    "    \"pageSize\": 2000\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the granuleids that are already accounted for (to avoid duplicates)\n",
    "hls_seen_granules = []\n",
    "\n",
    "# Initialize total size and number of granules\n",
    "total_size = 0\n",
    "num_granules = 0\n",
    "\n",
    "# Iterate through years 2019 - 2021\n",
    "for year in range(2019, 2022):\n",
    "    # For each of the polygon, we make a cmr call to get the granules, and add them to the total\n",
    "    # We use the hls_seen_granules list to avoid duplicates\n",
    "    for polygon in tqdm(polygons):\n",
    "        hls_dict[\"polygon\"] = polygon\n",
    "        hls_dict[\"temporal\"] = f\"{year}-06-01T00:00:00Z,{year}-09-15T23:59:59Z\",\n",
    "        result = get_size_estimates(hls_dict, host=\"nasa\", format=\"umm_json\", seen=hls_seen_granules)\n",
    "\n",
    "        total_size += result[\"total_size\"]\n",
    "        num_granules += result[\"count\"]\n",
    "\n",
    "hls_estimates = {\n",
    "    \"total_size\": total_size,\n",
    "    \"count\": num_granules\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all the estimates related to Boreal to get the total size estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_size': 140912001176.87454, 'count': 18469}\n"
     ]
    }
   ],
   "source": [
    "combined_boreal = [atl03_estimates, atl08_estimates, copernicus_estimates, hls_estimates]\n",
    "combined_boreal_estimates = {\n",
    "    \"total_size\": sum([x[\"total_size\"] for x in combined_boreal]),\n",
    "    \"count\": sum([x[\"count\"] for x in combined_boreal])\n",
    "}\n",
    "print(combined_boreal_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
